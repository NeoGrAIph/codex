diff --git a/codex-rs/ollama/src/client.rs b/codex-rs/ollama/src/client.rs
index a0ab2d04f..4f603c68b 100644
--- a/codex-rs/ollama/src/client.rs
+++ b/codex-rs/ollama/src/client.rs
@@ -13,6 +13,7 @@ use crate::url::base_url_to_host_root;
 use crate::url::is_openai_compatible_base_url;
 use codex_core::ModelProviderInfo;
 use codex_core::OLLAMA_OSS_PROVIDER_ID;
+use codex_core::WireApi;
 use codex_core::config::Config;
 
 const OLLAMA_CONNECTION_ERROR: &str = "No running Ollama server detected. Start it with: `ollama serve` (after installing). Install instructions: https://github.com/ollama/ollama?tab=readme-ov-file#ollama";
@@ -48,7 +49,7 @@ impl OllamaClient {
     #[cfg(test)]
     async fn try_from_provider_with_base_url(base_url: &str) -> io::Result<Self> {
         let provider =
-            codex_core::create_oss_provider_with_base_url(base_url, codex_core::WireApi::Responses);
+            codex_core::create_oss_provider_with_base_url(base_url, codex_core::WireApi::Chat);
         Self::try_from_provider(&provider).await
     }
 
@@ -59,7 +60,9 @@ impl OllamaClient {
             .base_url
             .as_ref()
             .expect("oss provider must have a base_url");
-        let uses_openai_compat = is_openai_compatible_base_url(base_url);
+        let uses_openai_compat = is_openai_compatible_base_url(base_url)
+            || matches!(provider.wire_api, WireApi::Chat)
+                && is_openai_compatible_base_url(base_url);
         let host_root = base_url_to_host_root(base_url);
         let client = reqwest::Client::builder()
             .connect_timeout(std::time::Duration::from_secs(5))
